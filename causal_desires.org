#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/style.css">
#+OPTIONS: num:nil toc:nil html-postamble:nil

#+TITLE: Causal Desires

#+HTML: <q> To model is human; causally, divine </q>

"Why?"

The question we -- children, scientists, scorned lovers, irate customers -- all love. We can't help ourselves! We need a story, ideally one that's easy to understand and fits with other stories. A little slice of reality where things happen as we expect, where we understand, where we could plan or possibly affect outcomes.

Some of us like to take this to extremes, engaging in formal study to learn some really good why-stories. Why-stories which are carefully framed, with well-probed boundaries and beautiful caveats. Universal why-stories or incredibly specific ones, couched in numbers or  careful analysis.

It is then no surprise that the art of why-storying itself is also formalized, so let us also be formal and call it causation. Aristotle loved it and students today, well, at least tolerate it and dutifully learn that causation is not correlation. 

To study causality when there's a lot happening -- lots of possible variables, observations, effects, it's easy to lose intuition. Pearl's causal calculus provides a scalable framework for drawing conclusions regarding causality [fn:1]. This approach is rigorous, but starts from the modeller specifying a graph which shows which variables can affect other variables. This makes explicit an uncomfortable point: causality is a model property. The causal calculus tells you some non-obvious fact about the assumptions you've made, but is subject to possible mismatch between those assumptions and reality -- a point nicely detailed in Norton's [[https://www.pitt.edu/~jdnorton/papers/003004.pdf]['Causation as Folk Science']]. 

The principle concern of causal calculus is formal decision making [fn:2]; questions like 'does smoking cause cancer' or 'does this intervention cause this health outcome'. This has two major distinctions from causal reasoning in our daily lives: it is laborious and it is a single statement. Decisions of daily life are messy, hasty, and often occur or re-occur over longer time periods. 

With repeated decisions, the situation is more complex: our actions affect our surroundings, which in turn affects us again. In sequential decisions - whether to go to the gym, whether to pick up flowers for your partner - our decisions shape our world and turn back, shaping our expectations, perceptions, and in turn our future decisions.

Consider a simple rational agent, choosing actions to achieve a desired outcome. Inside, this agent is using some model which translates an action into outcome, a counterfactual if-this-then-that which is one theory of mind [fn:4]. This forward model (action -> outcome) impacts the agent's actions, but may be incomplete. Specifically, it may be a simplified causal story which is missing how it affects us. 

For example, I feel warmth and love towards my partner and on the way home buy them flowers. However, such acts of love also heighten and draw attention to my feelings of love. Such acts also (probably) affect my partner and encourage reciprocal acts, which similarly amplify these feelings. As Nussbaum puts it, 'we inhabit our expressions' [fn:3].  

In that example, the incomplete story of feeling love -> buy flowers leads to the same decision as the more complex story. But, in general, an incomplete causal story leaves room for plausible stories which invert the causality - where the effect is presented as a cause. The self-fullfilling prophecy is a sort of causality inversion, positive visualization another.

Like any story, a causal story is a framing which centers certain facts and insights, and hides others. By being aware of our human need for a causal story, we can greet these stories of cause and effect with an appropriate amount of skepticism.  It may be that flipping the causality - considering the effect as a cause - provides useful insights.  

Some examples:

[fn:1] Pearl and Mackenzie, 'The Book of Why: The New Science of Cause and Effect', a number of shorter introductions [[https://www.inference.vc/untitled/][available]].
[fn:2] Although the causal graph can be extended to consider a time series, this is decidedly not the focus of the text.
[fn:3] Nussbaum, "Love and the Individual: Romantic Rightness and Platonic Aspiration" 
[fn:4] This theory of decision making is not the only one; see, e.g. [[https://arxiv.org/abs/1901.01291]['On the Utility of Model Learning in HRI']] by Choudhury, et al.

** Moore's law
Moore's law, the empirical rule of thumb that, in commercial semiconductor fabrication, the transistor count per chip doubles every two years. This observation, credited to Gordon Moore of Intel in 1965, was, at the time of its coinage, a model for predicting what computing power would be reasonable to expect in the future. This is a story which helps software developers, consumers, and corporations plan their investments.

However, as many have noted, Moore's law does more than predict: it is also a benchmark, setting the industry tempo, driving R&D direction and funding. 
 
** Eternal salvation
A prototypical religious view is that righteous actions cause eternal salvation. The Puritans, especially Calvinists, typically espouse a form of predestination: that one does not choose salvation, but rather that 'the privilege of choice is God's alone'. Good behavior *results* from being elected for salvation, but does not *cause* it.

How does such a viewpoint affect the person who holds it? Consider someone who has done an action they judge to be 'not righteous'. It seems they would take this as strong evidence that they are not elected for salvation, which could lead to a strong bifurcation in behavior - giving up.  

** Technology development
Let us say, for the sake of argument, that a technology is a means to reaching certain ends. Let's call this ability to do things an 'affordance'. What is a causal story of technology creation?  One is that someone invests resources in hopes of achieving an affordance. For example, we created a material that has a higher strength-to-weight ratio, or an algorithm which allows faster optimization of a class of problems. In this story, the technology is caused by a desire for an affordance.

However, some solutions are created for a specific need or application, where existing or new technologies are pressed into service for a specific problem. For example, I need to hold a door open and finds a book nearby to use, or I need to scale neural networks and create TPUs. In this story, technology is caused by a specific need.

These complementary stories are sometimes called basic (or curiosity-driven or push)  vs. application-driven (or pull) development. 

All development is a mixture of the two, but typically one dominates the framing for a funding agency, investor, or other decision maker. For example, the DARPA research model is a 'right-to-left' method (presumably technologies move from left to right as they move down a pipeline): look at what novel use-cases could fit their agency scope/objectives, then go 'shopping' in more basic research to find and foster supporting technologies.

This bi-directional causality of technical capability and need makes research and development difficult. This difficulty is exacerbated by the tendency of a person to stick to a single framing. For us engineers, the overwhelming tendency is to see technology development as the goal: to create something which meets certain specifications. But for a technology to become widespread, it also needs to meet a need.

The need can be brought to engineers in at least one of two ways.  In companies, explicit roles like product managers own those needs and communicate them to the engineers. For the academic research community, use cases are often left more vague as general visions (e.g. make robots easier to program), and a major skill is implicitly negotiating these with the community and funders.  
